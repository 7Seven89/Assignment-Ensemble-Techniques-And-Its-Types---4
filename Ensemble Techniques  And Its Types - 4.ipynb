{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8a7a6f0-abb3-4779-b984-4a1d631cf0a5",
   "metadata": {},
   "source": [
    "### Build a Random Forest Classifier to Predict the Risk of Heart Disease\n",
    "\n",
    "You are tasked with building a random forest classifier to predict the risk of heart disease based on a dataset containing patient information. The dataset contains 303 instances with 14 features, including age, sex, chest pain type, resting blood pressure, serum cholesterol, and maximum heart rate achieved.  \n",
    "Dataset link: [Google Drive](https://drive.google.com/file/d/1bGoIE4Z2kG5nyh-fGZAJ7LH0ki3UfmSJ/view?usp=share_link)\n",
    "\n",
    "### Q1. Preprocess the dataset by handling missing values, encoding categorical variables, and scaling the numerical features if necessary.\n",
    "\n",
    "### Q2. Split the dataset into a training set (70%) and a test set (30%).\n",
    "\n",
    "### Q3. Train a random forest classifier on the training set using 100 trees and a maximum depth of 10 for each tree. Use the default values for other hyperparameters.\n",
    "\n",
    "### Q4. Evaluate the performance of the model on the test set using accuracy, precision, recall, and F1 score.\n",
    "\n",
    "### Q5. Use the feature importance scores to identify the top 5 most important features in predicting heart disease risk. Visualise the feature importances using a bar chart.\n",
    "\n",
    "### Q6. Tune the hyperparameters of the random forest classifier using grid search or random search. Try different values of the number of trees, maximum depth, minimum samples split, and minimum samples leaf. Use 5-fold cross-validation to evaluate the performance of each set of hyperparameters.\n",
    "\n",
    "### Q7. Report the best set of hyperparameters found by the search and the corresponding performance metrics. Compare the performance of the tuned model with the default model.\n",
    "\n",
    "### Q8. Interpret the model by analysing the decision boundaries of the random forest classifier. Plot the decision boundaries on a scatter plot of two of the most important features. Discuss the insights and limitations of the model for predicting heart disease risk.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8ce174-b0a0-49c7-962e-6c4b4470aaff",
   "metadata": {},
   "source": [
    "## Step 1: Importing Libraries and Loading Data\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Load the dataset\n",
    "url = 'https://drive.google.com/uc?id=1bGoIE4Z2kG5nyh-fGZAJ7LH0ki3UfmSJ'\n",
    "df = pd.read_csv(url)\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b663b2-3ff0-447d-988d-dcf336a863dd",
   "metadata": {},
   "source": [
    "## Q1: Preprocessing the Dataset\n",
    "- Handling Missing Values: We'll use the SimpleImputer to handle any missing data by replacing them with the median (for numerical features).\n",
    "- Encoding Categorical Variables: We'll encode categorical features using OneHotEncoder.\n",
    "- Scaling Numerical Features: We'll scale the numerical features using StandardScaler to ensure they are on the same scale.\n",
    "\n",
    "```python\n",
    "# Check for missing values\n",
    "df.isnull().sum()\n",
    "\n",
    "# Preprocessing\n",
    "# Impute missing values\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "df[['age', 'trestbps', 'chol', 'thalach', 'oldpeak']] = imputer.fit_transform(df[['age', 'trestbps', 'chol', 'thalach', 'oldpeak']])\n",
    "\n",
    "# Encode categorical variables (e.g., 'sex', 'cp', 'fbs', etc.)\n",
    "df = pd.get_dummies(df, drop_first=True)\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(df.drop('target', axis=1))  # 'target' is the label column\n",
    "\n",
    "# Concatenate scaled features with the target variable\n",
    "X = pd.DataFrame(scaled_features, columns=df.columns[:-1])\n",
    "y = df['target']\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7411ea27-f56e-4799-91f8-8ebf55f1d2e3",
   "metadata": {},
   "source": [
    "## Q2: Splitting the Dataset\n",
    "We will split the dataset into training and testing sets (70% for training and 30% for testing).\n",
    "\n",
    "```python\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e942a446-6e59-4d23-a1b9-85533b222d03",
   "metadata": {},
   "source": [
    "## Q3: Train Random Forest Classifier\n",
    "Now, let's train a random forest classifier with 100 trees and a maximum depth of 10.\n",
    "\n",
    "```python\n",
    "# Train the Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3933a2e4-9a0f-4bc2-a08d-9e726d0e77f2",
   "metadata": {},
   "source": [
    "## Q4: Evaluate Model Performance\n",
    "We will evaluate the model using accuracy, precision, recall, and F1 score.\n",
    "\n",
    "```python\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca012f1-e404-46cf-8dc0-fee57b04ac82",
   "metadata": {},
   "source": [
    "## Q5: Feature Importance\n",
    "We will extract and visualize the feature importances to identify the top 5 most important features.\n",
    "\n",
    "```python\n",
    "# Get feature importance scores\n",
    "feature_importances = rf_classifier.feature_importances_\n",
    "\n",
    "# Create a DataFrame of features and their importance scores\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': feature_importances\n",
    "})\n",
    "\n",
    "# Sort features by importance\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot the top 5 most important features\n",
    "top_5_features = feature_importance_df.head(5)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=top_5_features)\n",
    "plt.title('Top 5 Important Features for Heart Disease Risk')\n",
    "plt.show()\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a24c12-870b-458d-b76b-8db3e165f926",
   "metadata": {},
   "source": [
    "## Q6: Hyperparameter Tuning\n",
    "We will tune the hyperparameters using Grid Search with 5-fold cross-validation.\n",
    "\n",
    "```python\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=RandomForestClassifier(random_state=42), param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best Hyperparameters: {best_params}\")\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7293dd79-6ee8-42fd-9824-a677a7f435e8",
   "metadata": {},
   "source": [
    "## Q7: Report the Best Set of Hyperparameters\n",
    "We will evaluate the model with the best hyperparameters and compare it with the default model.\n",
    "\n",
    "```python\n",
    "# Evaluate the best model from grid search\n",
    "best_rf = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions using the best model\n",
    "y_pred_best = best_rf.predict(X_test)\n",
    "\n",
    "# Evaluate the best model\n",
    "accuracy_best = accuracy_score(y_test, y_pred_best)\n",
    "precision_best = precision_score(y_test, y_pred_best)\n",
    "recall_best = recall_score(y_test, y_pred_best)\n",
    "f1_best = f1_score(y_test, y_pred_best)\n",
    "\n",
    "print(f\"Best Model Performance:\")\n",
    "print(f\"Accuracy: {accuracy_best}\")\n",
    "print(f\"Precision: {precision_best}\")\n",
    "print(f\"Recall: {recall_best}\")\n",
    "print(f\"F1 Score: {f1_best}\")\n",
    "\n",
    "# Compare with default model\n",
    "print(f\"Default Model F1 Score: {f1}\")\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655f5938-e5de-4358-a8a3-bc61685a2701",
   "metadata": {},
   "source": [
    "## Q8: Interpret the Model and Plot Decision Boundaries\n",
    "Since Random Forest is a non-linear model, it's difficult to interpret the decision boundaries directly. However, we can plot the decision boundaries for two important features to get some insights.\n",
    "\n",
    "```python\n",
    "# Select two important features for visualization\n",
    "top_features = top_5_features['Feature'][:2]\n",
    "X_subset = X[top_features]\n",
    "\n",
    "# Train the model on the two selected features\n",
    "rf_classifier.fit(X_subset, y)\n",
    "\n",
    "# Create a mesh grid for plotting decision boundaries\n",
    "xx, yy = np.meshgrid(np.linspace(X_subset.iloc[:, 0].min(), X_subset.iloc[:, 0].max(), 100),\n",
    "                     np.linspace(X_subset.iloc[:, 1].min(), X_subset.iloc[:, 1].max(), 100))\n",
    "\n",
    "# Predict on the mesh grid\n",
    "Z = rf_classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot decision boundary\n",
    "plt.contourf(xx, yy, Z, alpha=0.8)\n",
    "plt.scatter(X_subset.iloc[:, 0], X_subset.iloc[:, 1], c=y, edgecolors='k', marker='o')\n",
    "plt.xlabel(top_features[0])\n",
    "plt.ylabel(top_features[1])\n",
    "plt.title('Decision Boundary of Random Forest Classifier')\n",
    "plt.show()\n",
    "```\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
